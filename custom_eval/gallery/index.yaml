---
- &phi4
  url: "github:mudler/LocalAI/gallery/phi-4-chat.yaml@master"
  name: "phi-4"
  license: mit
  tags:
    - llm
    - gguf
    - phi
    - cpu
    - gpu
    - text-generation
  urls:
    - https://huggingface.co/microsoft/phi-4
    - https://huggingface.co/bartowski/phi-4-GGUF
  description: |
    phi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.
    phi-4 underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. Phi-4 is a 14B parameters, dense decoder-only Transformer model.
  overrides:
    parameters:
      model: phi-4-Q4_K_M.gguf
  files:
    - filename: phi-4-Q4_K_M.gguf
      uri: huggingface://bartowski/phi-4-GGUF/phi-4-Q4_K_M.gguf
      sha256: 009aba717c09d4a35890c7d35eb59d54e1dba884c7c526e7197d9c13ab5911d9
- &falcon3
  name: "falcon3-1b-instruct"
  url: "github:mudler/LocalAI/gallery/falcon3.yaml@master"
  icon: https://huggingface.co/datasets/tiiuae/documentation-images/resolve/main/general/falco3-logo.png
  urls:
    - https://huggingface.co/tiiuae/Falcon3-1B-Instruct
    - https://huggingface.co/bartowski/Falcon3-1B-Instruct-GGUF
  description: |
    Falcon3 family of Open Foundation Models is a set of pretrained and instruct LLMs ranging from 1B to 10B parameters.

    This repository contains the Falcon3-1B-Instruct. It achieves strong results on reasoning, language understanding, instruction following, code and mathematics tasks. Falcon3-1B-Instruct supports 4 languages (English, French, Spanish, Portuguese) and a context length of up to 8K.
  overrides:
    parameters:
      model: Falcon3-1B-Instruct-Q4_K_M.gguf
  files:
    - filename: Falcon3-1B-Instruct-Q4_K_M.gguf
      uri: huggingface://bartowski/Falcon3-1B-Instruct-GGUF/Falcon3-1B-Instruct-Q4_K_M.gguf
      sha256: 1c92013dac1ab6e703e787f3e0829ca03cc95311e4c113a77950d15ff6dea7b3
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - falcon
  license: falcon-llm
- !!merge <<: *falcon3
  name: "falcon3-3b-instruct"
  urls:
    - https://huggingface.co/tiiuae/Falcon3-3B-Instruct
    - https://huggingface.co/bartowski/Falcon3-3B-Instruct-GGUF
  overrides:
    parameters:
      model: Falcon3-3B-Instruct-Q4_K_M.gguf
  files:
    - filename: Falcon3-3B-Instruct-Q4_K_M.gguf
      uri: huggingface://bartowski/Falcon3-3B-Instruct-GGUF/Falcon3-3B-Instruct-Q4_K_M.gguf
      sha256: 6ea6cecba144fe5b711ca07ae4263ccdf6ee6419807a46220419189da8446557
- !!merge <<: *falcon3
  name: "falcon3-10b-instruct"
  urls:
    - https://huggingface.co/tiiuae/Falcon3-10B-Instruct
    - https://huggingface.co/bartowski/Falcon3-10B-Instruct-GGUF
  overrides:
    parameters:
      model: Falcon3-10B-Instruct-Q4_K_M.gguf
  files:
    - filename: Falcon3-10B-Instruct-Q4_K_M.gguf
      uri: huggingface://bartowski/Falcon3-10B-Instruct-GGUF/Falcon3-10B-Instruct-Q4_K_M.gguf
      sha256: 0a33327bd71e1788a8e9f17889824a17a65efd3f96a4b2a5e2bc6ff2f39b8241
- !!merge <<: *falcon3
  name: "falcon3-1b-instruct-abliterated"
  urls:
    - https://huggingface.co/huihui-ai/Falcon3-1B-Instruct-abliterated
    - https://huggingface.co/bartowski/Falcon3-1B-Instruct-abliterated-GGUF
  description: |
    This is an uncensored version of tiiuae/Falcon3-1B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).
    This is a crude, proof-of-concept implementation to remove refusals from an LLM model without using TransformerLens.
  overrides:
    parameters:
      model: Falcon3-1B-Instruct-abliterated-Q4_K_M.gguf
  files:
    - filename: Falcon3-1B-Instruct-abliterated-Q4_K_M.gguf
      sha256: 416d15ce58334b7956818befb088d46c1e3e7153ebf2da2fb9769a5b1ff934a1
      uri: huggingface://bartowski/Falcon3-1B-Instruct-abliterated-GGUF/Falcon3-1B-Instruct-abliterated-Q4_K_M.gguf
- !!merge <<: *falcon3
  name: "falcon3-3b-instruct-abliterated"
  urls:
    - https://huggingface.co/huihui-ai/Falcon3-3B-Instruct-abliterated
    - https://huggingface.co/bartowski/Falcon3-3B-Instruct-abliterated-GGUF
  description: |
    This is an uncensored version of tiiuae/Falcon3-3B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).
    This is a crude, proof-of-concept implementation to remove refusals from an LLM model without using TransformerLens.
  overrides:
    parameters:
      model: Falcon3-3B-Instruct-abliterated-Q4_K_M.gguf
  files:
    - filename: Falcon3-3B-Instruct-abliterated-Q4_K_M.gguf
      sha256: 83773b77b0e34ef115f8a6508192e9f1d3426a61456744493f65cfe1e7f90aa9
      uri: huggingface://bartowski/Falcon3-3B-Instruct-abliterated-GGUF/Falcon3-3B-Instruct-abliterated-Q4_K_M.gguf
- !!merge <<: *falcon3
  name: "falcon3-10b-instruct-abliterated"
  urls:
    - https://huggingface.co/huihui-ai/Falcon3-10B-Instruct-abliterated
    - https://huggingface.co/bartowski/Falcon3-10B-Instruct-abliterated-GGUF
  description: |
    This is an uncensored version of tiiuae/Falcon3-10B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).
    This is a crude, proof-of-concept implementation to remove refusals from an LLM model without using TransformerLens.
  overrides:
    parameters:
      model: Falcon3-10B-Instruct-abliterated-Q4_K_M.gguf
  files:
    - filename: Falcon3-10B-Instruct-abliterated-Q4_K_M.gguf
      sha256: 5940df2ff88e5be93dbe0766b2a9683d7e73c204a69a1348a37f835cf2b5f767
      uri: huggingface://bartowski/Falcon3-10B-Instruct-abliterated-GGUF/Falcon3-10B-Instruct-abliterated-Q4_K_M.gguf
- !!merge <<: *falcon3
  name: "falcon3-7b-instruct-abliterated"
  urls:
    - https://huggingface.co/huihui-ai/Falcon3-7B-Instruct-abliterated
    - https://huggingface.co/bartowski/Falcon3-7B-Instruct-abliterated-GGUF
  description: |
    This is an uncensored version of tiiuae/Falcon3-7B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).
    This is a crude, proof-of-concept implementation to remove refusals from an LLM model without using TransformerLens.
  overrides:
    parameters:
      model: Falcon3-7B-Instruct-abliterated-Q4_K_M.gguf
  files:
    - filename: Falcon3-7B-Instruct-abliterated-Q4_K_M.gguf
      sha256: 68e10e638668acaa49fb7919224c7d8bcf1798126c7a499c4d9ec3b81313f8c8
      uri: huggingface://bartowski/Falcon3-7B-Instruct-abliterated-GGUF/Falcon3-7B-Instruct-abliterated-Q4_K_M.gguf
- &intellect1
  name: "intellect-1-instruct"
  url: "github:mudler/LocalAI/gallery/llama3.1-instruct.yaml@master"
  icon: https://huggingface.co/PrimeIntellect/INTELLECT-1-Instruct/resolve/main/intellect-1-map.png
  urls:
    - https://huggingface.co/PrimeIntellect/INTELLECT-1-Instruct
    - https://huggingface.co/bartowski/INTELLECT-1-Instruct-GGUF
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - intellect
  license: apache-2.0
  description: |
    INTELLECT-1 is the first collaboratively trained 10 billion parameter language model trained from scratch on 1 trillion tokens of English text and code.
    This is an instruct model. The base model associated with it is INTELLECT-1.
    INTELLECT-1 was trained on up to 14 concurrent nodes distributed across 3 continents, with contributions from 30 independent community contributors providing compute. The training code utilizes the prime framework, a scalable distributed training framework designed for fault-tolerant, dynamically scaling, high-perfomance training on unreliable, globally distributed workers. The key abstraction that allows dynamic scaling is the ElasticDeviceMesh which manages dynamic global process groups for fault-tolerant communication across the internet and local process groups for communication within a node. The model was trained using the DiLoCo algorithms with 100 inner steps. The global all-reduce was done with custom int8 all-reduce kernels to reduce the communication payload required, greatly reducing the communication overhead by a factor 400x.
  overrides:
    parameters:
      model: INTELLECT-1-Instruct-Q4_K_M.gguf
  files:
    - filename: INTELLECT-1-Instruct-Q4_K_M.gguf
      sha256: 5df236fe570e5998d07fb3207788eac811ef3b77dd2a0ad04a2ef5c6361f3030
      uri: huggingface://bartowski/INTELLECT-1-Instruct-GGUF/INTELLECT-1-Instruct-Q4_K_M.gguf
- &llama33
  url: "github:mudler/LocalAI/gallery/llama3.1-instruct.yaml@master"
  icon: https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/aJJxKus1wP5N-euvHEUq7.png
  license: llama3.3
  description: |
    The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - llama3.3
  name: "llama-3.3-70b-instruct"
  urls:
    - https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
    - https://huggingface.co/MaziyarPanahi/Llama-3.3-70B-Instruct-GGUF
  overrides:
    parameters:
      model: Llama-3.3-70B-Instruct.Q4_K_M.gguf
  files:
    - filename: Llama-3.3-70B-Instruct.Q4_K_M.gguf
      sha256: 4f3b04ecae278bdb0fd545b47c210bc5edf823e5ebf7d41e0b526c81d54b1ff3
      uri: huggingface://MaziyarPanahi/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct.Q4_K_M.gguf
- !!merge <<: *llama33
  name: "l3.3-70b-euryale-v2.3"
  icon: https://huggingface.co/Sao10K/L3.3-70B-Euryale-v2.3/resolve/main/Eury.png
  urls:
    - https://huggingface.co/Sao10K/L3.3-70B-Euryale-v2.3
    - https://huggingface.co/bartowski/L3.3-70B-Euryale-v2.3-GGUF
  description: |
    A direct replacement / successor to Euryale v2.2, not Hanami-x1, though it is slightly better than them in my opinion.
  overrides:
    parameters:
      model: L3.3-70B-Euryale-v2.3-Q4_K_M.gguf
  files:
    - filename: L3.3-70B-Euryale-v2.3-Q4_K_M.gguf
      sha256: 4e78bb0e65886bfcff89b829f6d38aa6f6846988bb8291857e387e3f60b3217b
      uri: huggingface://bartowski/L3.3-70B-Euryale-v2.3-GGUF/L3.3-70B-Euryale-v2.3-Q4_K_M.gguf
- !!merge <<: *llama33
  name: "l3.3-ms-evayale-70b"
  icon: https://cdn-uploads.huggingface.co/production/uploads/64545af5ec40bbbd01242ca6/HFCaVzRpiE05Y46p41qRy.webp
  urls:
    - https://huggingface.co/Steelskull/L3.3-MS-Evayale-70B
    - https://huggingface.co/bartowski/L3.3-MS-Evayale-70B-GGUF
  description: |
    This model was created as I liked the storytelling of EVA but the prose and details of scenes from EURYALE, my goal is to merge the robust storytelling of both models while attempting to maintain the positives of both models.
  overrides:
    parameters:
      model: L3.3-MS-Evayale-70B-Q4_K_M.gguf
  files:
    - filename: L3.3-MS-Evayale-70B-Q4_K_M.gguf
      sha256: f941d88870fec8343946517a1802d159d23f3971eeea50b6cf12295330bd29cc
      uri: huggingface://bartowski/L3.3-MS-Evayale-70B-GGUF/L3.3-MS-Evayale-70B-Q4_K_M.gguf
- !!merge <<: *llama33
  name: "anubis-70b-v1"
  icon: https://cdn-uploads.huggingface.co/production/uploads/65f2fd1c25b848bd061b5c2e/qQbZvnrWYvH8dMZORLBJn.webp
  urls:
    - https://huggingface.co/TheDrummer/Anubis-70B-v1
    - https://huggingface.co/bartowski/Anubis-70B-v1-GGUF
  description: |
    It's a very balanced model between the L3.3 tunes. It's very creative, able to come up with new and interesting scenarios on your own that will thoroughly surprise you in ways that remind me of a 123B model. It has some of the most natural sounding dialogue and prose can come out of any model I've tried with the right swipe, in a way that truly brings your characters and RP to life that makes you feel like you're talking to a human writer instead of an AI - a quality that reminds me of Character AI in its prime. This model loves a great prompt and thrives off instructions.
  overrides:
    parameters:
      model: Anubis-70B-v1-Q4_K_M.gguf
  files:
    - filename: Anubis-70B-v1-Q4_K_M.gguf
      sha256: 9135f7090c675726469bd3a108cfbdddaa18638bad8e513928410de4b8bfd4d4
      uri: huggingface://bartowski/Anubis-70B-v1-GGUF/Anubis-70B-v1-Q4_K_M.gguf
- !!merge <<: *llama33
  name: "llama-3.3-70b-instruct-ablated"
  icon: https://cdn-uploads.huggingface.co/production/uploads/6587d8dd1b44d0e694104fbf/0dkt6EhZYwXVBxvSWXdaM.png
  urls:
    - https://huggingface.co/NaniDAO/Llama-3.3-70B-Instruct-ablated
    - https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-ablated-GGUF
  description: |
    Llama 3.3 instruct 70B 128k context with ablation technique applied for a more helpful (and based) assistant.

    This means it will refuse less of your valid requests for an uncensored UX. Use responsibly and use common sense.

    We do not take any responsibility for how you apply this intelligence, just as we do not for how you apply your own.
  overrides:
    parameters:
      model: Llama-3.3-70B-Instruct-ablated-Q4_K_M.gguf
  files:
    - filename: Llama-3.3-70B-Instruct-ablated-Q4_K_M.gguf
      sha256: 090b2288810c5f6f680ff5cb4bc97665393d115c011fcd54dca6aec02e74a983
      uri: huggingface://bartowski/Llama-3.3-70B-Instruct-ablated-GGUF/Llama-3.3-70B-Instruct-ablated-Q4_K_M.gguf
- !!merge <<: *llama33
  name: "l3.3-ms-evalebis-70b"
  icon: https://cdn-uploads.huggingface.co/production/uploads/64545af5ec40bbbd01242ca6/e49ykknqXee3Ihr-3BIl_.png
  urls:
    - https://huggingface.co/Steelskull/L3.3-MS-Evalebis-70b
    - https://huggingface.co/bartowski/L3.3-MS-Evalebis-70b-GGUF
  description: |
    This model was created as I liked the storytelling of EVA, the prose and details of scenes from EURYALE and Anubis, my goal is to merge the robust storytelling of all three models while attempting to maintain the positives of the models.
  overrides:
    parameters:
      model: L3.3-MS-Evalebis-70b-Q4_K_M.gguf
  files:
    - filename: L3.3-MS-Evalebis-70b-Q4_K_M.gguf
      sha256: 5515110ab6a583f6eb360533e3c5b3dda6d402af407c0b0f2b34a2a57b5224d5
      uri: huggingface://bartowski/L3.3-MS-Evalebis-70b-GGUF/L3.3-MS-Evalebis-70b-Q4_K_M.gguf
- !!merge <<: *llama33
  name: "rombos-llm-70b-llama-3.3"
  icon: "https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/QErypCEKD5OZLxUcSmYaR.jpeg"
  urls:
    - https://huggingface.co/rombodawg/Rombos-LLM-70b-Llama-3.3
    - https://huggingface.co/bartowski/Rombos-LLM-70b-Llama-3.3-GGUF
    - https://docs.google.com/document/d/1OjbjU5AOz4Ftn9xHQrX3oFQGhQ6RDUuXQipnQ9gn6tU/edit?usp=sharing
  description: |
    You know the drill by now.
    Here is the paper. Have fun.
    https://docs.google.com/document/d/1OjbjU5AOz4Ftn9xHQrX3oFQGhQ6RDUuXQipnQ9gn6tU/edit?usp=sharing
  overrides:
    parameters:
      model: Rombos-LLM-70b-Llama-3.3-Q4_K_M.gguf
  files:
    - filename: Rombos-LLM-70b-Llama-3.3-Q4_K_M.gguf
      uri: huggingface://bartowski/Rombos-LLM-70b-Llama-3.3-GGUF/Rombos-LLM-70b-Llama-3.3-Q4_K_M.gguf
      sha256: 613008b960f6fff346b5dec71a87cd7ecdaff205bfea6332bd8fe2bb46177352
- !!merge <<: *llama33
  name: "70b-l3.3-cirrus-x1"
  icon: https://huggingface.co/Sao10K/70B-L3.3-Cirrus-x1/resolve/main/venti.png
  urls:
    - https://huggingface.co/Sao10K/70B-L3.3-Cirrus-x1
    - https://huggingface.co/bartowski/70B-L3.3-Cirrus-x1-GGUF
  description: |
    - Same data composition as Freya, applied differently, trained longer too.
    - Merging with its checkpoints was also involved.
    - Has a nice style, with occasional issues that can be easily fixed.
    - A more stable version compared to previous runs.
  overrides:
    parameters:
      model: 70B-L3.3-Cirrus-x1-Q4_K_M.gguf
  files:
    - filename: 70B-L3.3-Cirrus-x1-Q4_K_M.gguf
      sha256: 07dd464dddba959df8eb2f937787c2210b4c51c2375bd7c7ab2abbe198142a19
      uri: huggingface://bartowski/70B-L3.3-Cirrus-x1-GGUF/70B-L3.3-Cirrus-x1-Q4_K_M.gguf
- &rwkv
  url: "github:mudler/LocalAI/gallery/rwkv.yaml@master"
  name: "rwkv-6-world-7b"
  license: apache-2.0
  urls:
    - https://huggingface.co/RWKV/rwkv-6-world-7b
    - https://huggingface.co/bartowski/rwkv-6-world-7b-GGUF
  tags:
    - llm
    - rwkv
    - cpu
    - gpu
    - rnn
  description: |
    RWKV (pronounced RwaKuv) is an RNN with GPT-level LLM performance, and can also be directly trained like a GPT transformer (parallelizable). We are at RWKV-7.
    So it's combining the best of RNN and transformer - great performance, fast inference, fast training, saves VRAM, "infinite" ctxlen, and free text embedding. Moreover it's 100% attention-free, and a Linux Foundation AI project.
  overrides:
    parameters:
      model: rwkv-6-world-7b-Q4_K_M.gguf
  files:
    - filename: rwkv-6-world-7b-Q4_K_M.gguf
      sha256: f74574186fa4584f405e92198605680db6ad00fd77974ffa14bf02073bb90273
      uri: huggingface://bartowski/rwkv-6-world-7b-GGUF/rwkv-6-world-7b-Q4_K_M.gguf
- &qwen25coder
  name: "qwen2.5-coder-14b"
  url: "github:mudler/LocalAI/gallery/chatml.yaml@master"
  license: apache-2.0
  tags:
    - llm
    - gguf
    - gpu
    - qwen
    - qwen2.5
    - cpu
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-14B
    - https://huggingface.co/mradermacher/Qwen2.5-Coder-14B-GGUF
  description: |
    Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:

        Significantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.
        A more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.
        Long-context Support up to 128K tokens.
  overrides:
    parameters:
      model: Qwen2.5-Coder-14B.Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-14B.Q4_K_M.gguf
      sha256: 94f277a9ac7caf117140b2fff4e1ccf4bc9f35395b0112f0d0d7c82c6f8d860e
      uri: huggingface://mradermacher/Qwen2.5-Coder-14B-GGUF/Qwen2.5-Coder-14B.Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "qwen2.5-coder-3b-instruct"
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct
    - https://huggingface.co/bartowski/Qwen2.5-Coder-3B-Instruct-GGUF
  overrides:
    parameters:
      model: Qwen2.5-Coder-3B-Instruct-Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-3B-Instruct-Q4_K_M.gguf
      sha256: 3da3afe6cf5c674ac195803ea0dd6fee7e1c228c2105c1ce8c66890d1d4ab460
      uri: huggingface://bartowski/Qwen2.5-Coder-3B-Instruct-GGUF/Qwen2.5-Coder-3B-Instruct-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "qwen2.5-coder-32b-instruct"
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct
    - https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF
  overrides:
    parameters:
      model: Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf
      sha256: 8e2fd78ff55e7cdf577fda257bac2776feb7d73d922613caf35468073807e815
      uri: huggingface://bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "qwen2.5-coder-14b-instruct"
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct
    - https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-GGUF
  overrides:
    parameters:
      model: Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf
      sha256: 2946d28c9e1bb2bcae6d42e8678863a31775df6f740315c7d7e6d6b6411f5937
      uri: huggingface://bartowski/Qwen2.5-Coder-14B-Instruct-GGUF/Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "qwen2.5-coder-1.5b-instruct"
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct
    - https://huggingface.co/bartowski/Qwen2.5-Coder-1.5B-Instruct-GGUF
  overrides:
    parameters:
      model: Qwen2.5-Coder-1.5B-Instruct-Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-1.5B-Instruct-Q4_K_M.gguf
      sha256: f530705d447660a4336c329981af164b471b60b974b1d808d57e8ec9fe23b239
      uri: huggingface://bartowski/Qwen2.5-Coder-1.5B-Instruct-GGUF/Qwen2.5-Coder-1.5B-Instruct-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "qwen2.5-coder-7b-instruct"
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct
    - https://huggingface.co/bartowski/Qwen2.5-Coder-7B-Instruct-GGUF
  overrides:
    parameters:
      model: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf
      sha256: 1664fccab734674a50763490a8c6931b70e3f2f8ec10031b54806d30e5f956b6
      uri: huggingface://bartowski/Qwen2.5-Coder-7B-Instruct-GGUF/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "qwen2.5-coder-7b-3x-instruct-ties-v1.2-i1"
  urls:
    - https://huggingface.co/BenevolenceMessiah/Qwen2.5-Coder-7B-3x-Instruct-TIES-v1.2
    - https://huggingface.co/mradermacher/Qwen2.5-Coder-7B-3x-Instruct-TIES-v1.2-i1-GGUF
  description: |
    The following models were included in the merge:
        BenevolenceMessiah/Qwen2.5-Coder-7B-Chat-Instruct-TIES-v1.2
        MadeAgents/Hammer2.0-7b
        huihui-ai/Qwen2.5-Coder-7B-Instruct-abliterated
  overrides:
    parameters:
      model: Qwen2.5-Coder-7B-3x-Instruct-TIES-v1.2.i1-Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-7B-3x-Instruct-TIES-v1.2.i1-Q4_K_M.gguf
      sha256: c28a4da700f634f1277f02391d81fa3c0ba783fa4b02886bd4bfe5f13b6605ef
      uri: huggingface://mradermacher/Qwen2.5-Coder-7B-3x-Instruct-TIES-v1.2-i1-GGUF/Qwen2.5-Coder-7B-3x-Instruct-TIES-v1.2.i1-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "qwen2.5-coder-7b-instruct-abliterated-i1"
  urls:
    - https://huggingface.co/huihui-ai/Qwen2.5-Coder-7B-Instruct-abliterated
    - https://huggingface.co/mradermacher/Qwen2.5-Coder-7B-Instruct-abliterated-i1-GGUF
  description: |
    This is an uncensored version of Qwen2.5-Coder-7B-Instruct created with abliteration (see this article to know more about it).

    Special thanks to @FailSpy for the original code and technique. Please follow him if you're interested in abliterated models.
  overrides:
    parameters:
      model: Qwen2.5-Coder-7B-Instruct-abliterated.i1-Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-7B-Instruct-abliterated.i1-Q4_K_M.gguf
      sha256: 9100ccd9e8167cefda98bd1c97d5d765a21e70e124e4d6b89945fd66ebb481b4
      uri: huggingface://mradermacher/Qwen2.5-Coder-7B-Instruct-abliterated-i1-GGUF/Qwen2.5-Coder-7B-Instruct-abliterated.i1-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "rombos-coder-v2.5-qwen-7b"
  icon: https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/QErypCEKD5OZLxUcSmYaR.jpeg
  urls:
    - https://huggingface.co/rombodawg/Rombos-Coder-V2.5-Qwen-7b
    - https://huggingface.co/bartowski/Rombos-Coder-V2.5-Qwen-7b-GGUF
    - https://docs.google.com/document/d/1OjbjU5AOz4Ftn9xHQrX3oFQGhQ6RDUuXQipnQ9gn6tU/edit?usp=sharing
  description: |
    Rombos-Coder-V2.5-Qwen-7b is a continues finetuned version of Qwen2.5-Coder-7B-Instruct. I took it upon myself to merge the instruct model with the base model myself using the * Ties* merge method as demonstrated in my own "Continuous Finetuning" method (link available).
    This version of the model shows higher performance than the original instruct and base models.
  overrides:
    parameters:
      model: Rombos-Coder-V2.5-Qwen-7b-Q4_K_M.gguf
  files:
    - filename: Rombos-Coder-V2.5-Qwen-7b-Q4_K_M.gguf
      sha256: ca16a550f1be00b7e92f94c0c18ea6af1e5c158d5d1cb3994f9f0a0d13922272
      uri: huggingface://bartowski/Rombos-Coder-V2.5-Qwen-7b-GGUF/Rombos-Coder-V2.5-Qwen-7b-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "rombos-coder-v2.5-qwen-32b"
  icon: https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/QErypCEKD5OZLxUcSmYaR.jpeg
  urls:
    - https://huggingface.co/rombodawg/Rombos-Coder-V2.5-Qwen-32b
    - https://huggingface.co/bartowski/Rombos-Coder-V2.5-Qwen-32b-GGUF
    - https://docs.google.com/document/d/1OjbjU5AOz4Ftn9xHQrX3oFQGhQ6RDUuXQipnQ9gn6tU/edit?usp=sharing
  description: |
    Rombos-Coder-V2.5-Qwen-32b is a continues finetuned version of Qwen2.5-Coder-32B-Instruct. I took it upon myself to merge the instruct model with the base model myself using the Ties merge method as demonstrated in my own "Continuous Finetuning" method (link available).
    This version of the model shows higher performance than the original instruct and base models.
  overrides:
    parameters:
      model: Rombos-Coder-V2.5-Qwen-32b-Q4_K_M.gguf
  files:
    - filename: Rombos-Coder-V2.5-Qwen-32b-Q4_K_M.gguf
      sha256: 821ea2a13d96354db1368986700b1189938fbbc56ca6bb9d0c39f752580de71a
      uri: huggingface://bartowski/Rombos-Coder-V2.5-Qwen-32b-GGUF/Rombos-Coder-V2.5-Qwen-32b-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "rombos-coder-v2.5-qwen-14b"
  icon: https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/QErypCEKD5OZLxUcSmYaR.jpeg
  urls:
    - https://huggingface.co/rombodawg/Rombos-Coder-V2.5-Qwen-14b
    - https://huggingface.co/bartowski/Rombos-Coder-V2.5-Qwen-14b-GGUF
    - https://docs.google.com/document/d/1OjbjU5AOz4Ftn9xHQrX3oFQGhQ6RDUuXQipnQ9gn6tU/edit?usp=sharing
  description: |
    Rombos-Coder-V2.5-Qwen-14b is a continues finetuned version of Qwen2.5-Coder-14B-Instruct. I took it upon myself to merge the instruct model with the base model myself using the Ties merge method as demonstrated in my own "Continuous Finetuning" method (link available).
    This version of the model shows higher performance than the original instruct and base models.
  overrides:
    parameters:
      model: Rombos-Coder-V2.5-Qwen-14b-Q4_K_M.gguf
  files:
    - filename: Rombos-Coder-V2.5-Qwen-14b-Q4_K_M.gguf
      sha256: 7ef044e1fee206a039f56538f94332030e99ec63915c74f4d1bdec0e601ee968
      uri: huggingface://bartowski/Rombos-Coder-V2.5-Qwen-14b-GGUF/Rombos-Coder-V2.5-Qwen-14b-Q4_K_M.gguf
- !!merge <<: *qwen25coder
  name: "qwen2.5-coder-32b-instruct-uncensored-i1"
  urls:
    - https://huggingface.co/thirdeyeai/Qwen2.5-Coder-32B-Instruct-Uncensored
    - https://huggingface.co/mradermacher/Qwen2.5-Coder-32B-Instruct-Uncensored-i1-GGUF
  description: |
    The LLM model is based on sloshywings/Qwen2.5-Coder-32B-Instruct-Uncensored. It is a large language model with 32B parameters that has been fine-tuned on coding tasks and instructions.
  overrides:
    parameters:
      model: Qwen2.5-Coder-32B-Instruct-Uncensored.i1-Q4_K_M.gguf
  files:
    - filename: Qwen2.5-Coder-32B-Instruct-Uncensored.i1-Q4_K_M.gguf
      sha256: 86ac8efb86daf241792ac3d5d35b7da92c54901b4208a6f2829bd03d8f273c9c
      uri: huggingface://mraWdermacher/Qwen2.5-Coder-32B-Instruct-Uncensored-i1-GGUF/Qwen2.5-Coder-32B-Instruct-Uncensored.i1-Q4_K_M.gguf
- &opencoder
  name: "opencoder-8b-base"
  icon: https://github.com/OpenCoder-llm/opencoder-llm.github.io/blob/main/static/images/opencoder_icon.jpg?raw=true
  url: "github:mudler/LocalAI/gallery/codellama.yaml@master"
  urls:
    - https://huggingface.co/infly/OpenCoder-8B-Base
    - https://huggingface.co/QuantFactory/OpenCoder-8B-Base-GGUF
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - code
  license: inf
  description: |
    The model is a quantized version of infly/OpenCoder-8B-Base created using llama.cpp. It is part of the OpenCoder LLM family which includes 1.5B and 8B base and chat models, supporting both English and Chinese languages. The original OpenCoder model was pretrained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, and supervised finetuned on over 4.5M high-quality SFT examples. It achieves high performance across multiple language model benchmarks and is one of the most comprehensively open-sourced models available.
  overrides:
    parameters:
      model: OpenCoder-8B-Base.Q4_K_M.gguf
  files:
    - filename: OpenCoder-8B-Base.Q4_K_M.gguf
      sha256: ed158a6f72a40cf4f3f4569f649b365f5851e93f03b56252af3906515fab94ec
      uri: huggingface://QuantFactory/OpenCoder-8B-Base-GGUF/OpenCoder-8B-Base.Q4_K_M.gguf
- !!merge <<: *opencoder
  url: "github:mudler/LocalAI/gallery/hermes-2-pro-mistral.yaml@master"
  name: "opencoder-8b-instruct"
  urls:
    - https://huggingface.co/infly/OpenCoder-8B-Instruct
    - https://huggingface.co/QuantFactory/OpenCoder-8B-Instruct-GGUF
  description: |
    The LLM model is QuantFactory/OpenCoder-8B-Instruct-GGUF, which is a quantized version of infly/OpenCoder-8B-Instruct. It is created using llama.cpp and supports both English and Chinese languages. The original model, infly/OpenCoder-8B-Instruct, is pretrained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, and supervised finetuned on over 4.5M high-quality SFT examples. It achieves high performance across multiple language model benchmarks and is one of the leading open-source models for code.
  overrides:
    parameters:
      model: OpenCoder-8B-Instruct.Q4_K_M.gguf
  files:
    - filename: OpenCoder-8B-Instruct.Q4_K_M.gguf
      sha256: ae642656f127e339fcb9566e6039a73cc55d34e3bf59e067d58ad40742f49f00
      uri: huggingface://QuantFactory/OpenCoder-8B-Instruct-GGUF/OpenCoder-8B-Instruct.Q4_K_M.gguf
- !!merge <<: *opencoder
  name: "opencoder-1.5b-base"
  urls:
    - https://huggingface.co/infly/OpenCoder-1.5B-Base
    - https://huggingface.co/QuantFactory/OpenCoder-1.5B-Base-GGUF
  description: |
    The model is a large language model with 1.5 billion parameters, trained on 2.5 trillion tokens of code-related data. It supports both English and Chinese languages and is part of the OpenCoder LLM family which also includes 8B base and chat models. The model achieves high performance across multiple language model benchmarks and is one of the most comprehensively open-sourced models available.
  overrides:
    parameters:
      model: OpenCoder-1.5B-Base.Q4_K_M.gguf
  files:
    - filename: OpenCoder-1.5B-Base.Q4_K_M.gguf
      sha256: fb69a2849971b69f3fa1e64a17d1e4d3e1d0d3733d43ae8645299d07ab855af5
      uri: huggingface://QuantFactory/OpenCoder-1.5B-Base-GGUF/OpenCoder-1.5B-Base.Q4_K_M.gguf
- !!merge <<: *opencoder
  name: "opencoder-1.5b-instruct"
  url: "github:mudler/LocalAI/gallery/hermes-2-pro-mistral.yaml@master"
  urls:
    - https://huggingface.co/QuantFactory/OpenCoder-1.5B-Instruct-GGUF
  description: |
    The model is a quantized version of [infly/OpenCoder-1.5B-Instruct](https://huggingface.co/infly/OpenCoder-1.5B-Instruct) created using llama.cpp. The original model, infly/OpenCoder-1.5B-Instruct, is an open and reproducible code LLM family which includes 1.5B and 8B base and chat models, supporting both English and Chinese languages. The model is pretrained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, and supervised finetuned on over 4.5M high-quality SFT examples. It achieves high performance across multiple language model benchmarks, positioning it among the leading open-source models for code.
  overrides:
    parameters:
      model: OpenCoder-1.5B-Instruct.Q4_K_M.gguf
  files:
    - filename: OpenCoder-1.5B-Instruct.Q4_K_M.gguf
      sha256: a34128fac79e05a3a92c3fd2245cfce7c3876c60241ec2565c24e74b36f48d56
      uri: huggingface://QuantFactory/OpenCoder-1.5B-Instruct-GGUF/OpenCoder-1.5B-Instruct.Q4_K_M.gguf
- &granite3
  name: "granite-3.0-1b-a400m-instruct"
  urls:
    - https://huggingface.co/ibm-granite/granite-3.0-1b-a400m-instruct
    - https://huggingface.co/QuantFactory/granite-3.0-1b-a400m-instruct-GGUF
  overrides:
    parameters:
      model: granite-3.0-1b-a400m-instruct.Q4_K_M.gguf
  files:
    - filename: granite-3.0-1b-a400m-instruct.Q4_K_M.gguf
      sha256: 9571b5fc9676ebb59def3377dc848584463fb7f09ed59ebbff3b9f72fd7bd38a
      uri: huggingface://QuantFactory/granite-3.0-1b-a400m-instruct-GGUF/granite-3.0-1b-a400m-instruct.Q4_K_M.gguf
  url: "github:mudler/LocalAI/gallery/granite.yaml@master"
  description: |
    Granite 3.0 language models are a new set of lightweight state-of-the-art, open foundation models that natively support multilinguality, coding, reasoning, and tool usage, including the potential to be run on constrained compute resources. All the models are publicly released under an Apache 2.0 license for both research and commercial use. The models' data curation and training procedure were designed for enterprise usage and customization in mind, with a process that evaluates datasets for governance, risk and compliance (GRC) criteria, in addition to IBM's standard data clearance process and document quality checks.
    Granite 3.0 includes 4 different models of varying sizes:
        Dense Models: 2B and 8B parameter models, trained on 12 trillion tokens in total.
        Mixture-of-Expert (MoE) Models: Sparse 1B and 3B MoE models, with 400M and 800M activated parameters respectively, trained on 10 trillion tokens in total.
    Accordingly, these options provide a range of models with different compute requirements to choose from, with appropriate trade-offs with their performance on downstream tasks. At each scale, we release a base model — checkpoints of models after pretraining, as well as instruct checkpoints — models finetuned for dialogue, instruction-following, helpfulness, and safety.
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - moe
    - granite
- !!merge <<: *granite3
  name: "moe-girl-800ma-3bt"
  icon: https://huggingface.co/allura-org/MoE-Girl-800MA-3BT/resolve/main/moe-girl-800-3.png
  url: "github:mudler/LocalAI/gallery/chatml.yaml@master"
  urls:
    - https://huggingface.co/allura-org/MoE-Girl-800MA-3BT
    - https://huggingface.co/mradermacher/MoE-Girl-800MA-3BT-GGUF
  description: |
    A roleplay-centric finetune of IBM's Granite 3.0 3B-A800M. LoRA finetune trained locally, whereas the others were FFT; while this results in less uptake of training data, it should also mean less degradation in Granite's core abilities, making it potentially easier to use for general-purpose tasks.
    Disclaimer

    PLEASE do not expect godliness out of this, it's a model with 800 million active parameters. Expect something more akin to GPT-3 (the original, not GPT-3.5.) (Furthermore, this version is by a less experienced tuner; it's my first finetune that actually has decent-looking graphs, I don't really know what I'm doing yet!)
  overrides:
    parameters:
      model: MoE-Girl-800MA-3BT.Q4_K_M.gguf
  files:
    - filename: MoE-Girl-800MA-3BT.Q4_K_M.gguf
      sha256: 4c3cb57c27aadabd05573a1a01d6c7aee0f21620db919c7704f758d172e0bfa3
      uri: huggingface://mradermacher/MoE-Girl-800MA-3BT-GGUF/MoE-Girl-800MA-3BT.Q4_K_M.gguf
